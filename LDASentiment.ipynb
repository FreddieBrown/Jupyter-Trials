{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA \n",
    "[Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a technique by which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Conv1D, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data = pd.read_csv(\"data/Reviews.csv\", nrows=10000)\n",
    "data_text = pd.DataFrame(data[\"Text\"])\n",
    "data_text['index'] = data_text.index\n",
    "data_text.head()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_and_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos = 'v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_and_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [buy, vital, can, food, product, good, qualiti...\n",
       "1    [product, arriv, label, jumbo, salt, peanut, p...\n",
       "2    [confect, centuri, light, pillowi, citrus, gel...\n",
       "3    [look, secret, ingredi, robitussin, believ, ad...\n",
       "4    [great, taffi, great, price, wide, assort, yum...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = data_text[\"Text\"].map(preprocess)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 2),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 3),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1)],\n",
       " [(11, 2),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 2),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"appreci\") appears 1 time.\n",
      "Word 1 (\"better\") appears 2 time.\n",
      "Word 2 (\"buy\") appears 1 time.\n",
      "Word 3 (\"can\") appears 1 time.\n",
      "Word 4 (\"finicki\") appears 1 time.\n",
      "Word 5 (\"food\") appears 1 time.\n",
      "Word 6 (\"good\") appears 1 time.\n",
      "Word 7 (\"like\") appears 1 time.\n",
      "Word 8 (\"look\") appears 1 time.\n",
      "Word 9 (\"meat\") appears 1 time.\n",
      "Word 10 (\"process\") appears 1 time.\n",
      "Word 11 (\"product\") appears 3 time.\n",
      "Word 12 (\"qualiti\") appears 1 time.\n",
      "Word 13 (\"smell\") appears 1 time.\n",
      "Word 14 (\"stew\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_0 = bow_corpus[0]\n",
    "\n",
    "for i in range(len(bow_doc_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "                                                     dictionary[bow_doc_0[i][0]], \n",
    "                                                     bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.043*\"coffe\" + 0.026*\"flavor\" + 0.020*\"tast\" + 0.018*\"like\" + 0.013*\"good\" + 0.012*\"love\" + 0.011*\"drink\" + 0.011*\"product\" + 0.009*\"great\" + 0.009*\"tri\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.020*\"like\" + 0.016*\"tast\" + 0.014*\"product\" + 0.014*\"good\" + 0.012*\"great\" + 0.010*\"love\" + 0.009*\"flavor\" + 0.008*\"food\" + 0.007*\"time\" + 0.007*\"order\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=2, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\\n\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.012*\"coffe\" + 0.009*\"flavor\" + 0.009*\"tast\" + 0.007*\"good\" + 0.006*\"product\" + 0.005*\"like\" + 0.005*\"vanilla\" + 0.005*\"love\" + 0.005*\"tri\" + 0.005*\"drink\"\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.017*\"coffe\" + 0.008*\"flavor\" + 0.007*\"like\" + 0.007*\"great\" + 0.007*\"love\" + 0.006*\"cup\" + 0.006*\"tast\" + 0.006*\"order\" + 0.005*\"good\" + 0.005*\"product\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8525947332382202\t \n",
      "Topic 1: 0.020*\"like\" + 0.016*\"tast\" + 0.014*\"product\" + 0.014*\"good\" + 0.012*\"great\" + 0.010*\"love\" + 0.009*\"flavor\" + 0.008*\"food\" + 0.007*\"time\" + 0.007*\"order\"\n",
      "\n",
      "Score: 0.14740528166294098\t \n",
      "Topic 0: 0.043*\"coffe\" + 0.026*\"flavor\" + 0.020*\"tast\" + 0.018*\"like\" + 0.013*\"good\" + 0.012*\"love\" + 0.011*\"drink\" + 0.011*\"product\" + 0.009*\"great\" + 0.009*\"tri\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[45]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic {}: {}\".format(score, index, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9146600961685181\t \n",
      "Topic 1: 0.017*\"coffe\" + 0.008*\"flavor\" + 0.007*\"like\" + 0.007*\"great\" + 0.007*\"love\" + 0.006*\"cup\" + 0.006*\"tast\" + 0.006*\"order\" + 0.005*\"good\" + 0.005*\"product\"\n",
      "\n",
      "Score: 0.08533991128206253\t \n",
      "Topic 0: 0.012*\"coffe\" + 0.009*\"flavor\" + 0.009*\"tast\" + 0.007*\"good\" + 0.006*\"product\" + 0.005*\"like\" + 0.005*\"vanilla\" + 0.005*\"love\" + 0.005*\"tri\" + 0.005*\"drink\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic {}: {}\".format(score, index, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for text in bow_corpus:\n",
    "    index, score = sorted(lda_model[text], key=lambda tup: -1*tup[1])[0]\n",
    "    classes.append(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = [\" \".join(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_text, classes, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "size = 200\n",
    "glove_file = open('glove.6B/glove.6B.'+str(size)+'d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, size))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Use neural networks to predict the sentiment of bodies of text. These are across two topics which were generated using LDA.\n",
    "\n",
    "## Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, size, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 100, 200)          2046400   \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 2,066,401\n",
      "Trainable params: 20,001\n",
      "Non-trainable params: 2,046,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.5600 - acc: 0.7191 - val_loss: 0.4897 - val_acc: 0.7794\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3996 - acc: 0.8506 - val_loss: 0.4403 - val_acc: 0.8106\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.3384 - acc: 0.8805 - val_loss: 0.4212 - val_acc: 0.8144\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.3024 - acc: 0.8950 - val_loss: 0.4087 - val_acc: 0.8250\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2752 - acc: 0.9098 - val_loss: 0.4029 - val_acc: 0.8225\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2553 - acc: 0.9141 - val_loss: 0.3995 - val_acc: 0.8194\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4010 - acc: 0.8275\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 100, 200)          2046400   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 96, 128)           128128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,174,657\n",
      "Trainable params: 128,257\n",
      "Non-trainable params: 2,046,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, size, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "50/50 [==============================] - 2s 40ms/step - loss: 0.4452 - acc: 0.7937 - val_loss: 0.3467 - val_acc: 0.8544\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 0.2700 - acc: 0.8948 - val_loss: 0.3156 - val_acc: 0.8712\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 2s 40ms/step - loss: 0.2006 - acc: 0.9320 - val_loss: 0.2997 - val_acc: 0.8750\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 2s 39ms/step - loss: 0.1530 - acc: 0.9573 - val_loss: 0.2855 - val_acc: 0.8756\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 2s 42ms/step - loss: 0.1130 - acc: 0.9759 - val_loss: 0.2890 - val_acc: 0.8744\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 0.0836 - acc: 0.9884 - val_loss: 0.2812 - val_acc: 0.8750\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2756 - acc: 0.8825\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrant NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 100, 200)          2046400   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,214,977\n",
      "Trainable params: 168,577\n",
      "Non-trainable params: 2,046,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, size, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6312 - acc: 0.6527 - val_loss: 0.6740 - val_acc: 0.4425\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6187 - acc: 0.6706 - val_loss: 0.5965 - val_acc: 0.7200\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.5782 - acc: 0.7369 - val_loss: 0.5916 - val_acc: 0.7200\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.5785 - acc: 0.7358 - val_loss: 0.6091 - val_acc: 0.7056\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.5906 - acc: 0.7192 - val_loss: 0.6066 - val_acc: 0.7044\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5916 - acc: 0.7186 - val_loss: 0.6030 - val_acc: 0.7050\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.5874 - acc: 0.7225\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=50, verbose=1, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

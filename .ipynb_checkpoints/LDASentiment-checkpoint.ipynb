{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Conv1D, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "data = pd.read_csv(\"data/Reviews.csv\", nrows=10000)\n",
    "data_text = pd.DataFrame(data[\"Text\"])\n",
    "data_text['index'] = data_text.index\n",
    "data_text.head()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_and_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos = 'v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_and_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [buy, vital, can, food, product, good, qualiti...\n",
       "1    [product, arriv, label, jumbo, salt, peanut, p...\n",
       "2    [confect, centuri, light, pillowi, citrus, gel...\n",
       "3    [look, secret, ingredi, robitussin, believ, ad...\n",
       "4    [great, taffi, great, price, wide, assort, yum...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = data_text[\"Text\"].map(preprocess)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 2),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 3),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1)],\n",
       " [(11, 2),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 2),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"appreci\") appears 1 time.\n",
      "Word 1 (\"better\") appears 2 time.\n",
      "Word 2 (\"buy\") appears 1 time.\n",
      "Word 3 (\"can\") appears 1 time.\n",
      "Word 4 (\"finicki\") appears 1 time.\n",
      "Word 5 (\"food\") appears 1 time.\n",
      "Word 6 (\"good\") appears 1 time.\n",
      "Word 7 (\"like\") appears 1 time.\n",
      "Word 8 (\"look\") appears 1 time.\n",
      "Word 9 (\"meat\") appears 1 time.\n",
      "Word 10 (\"process\") appears 1 time.\n",
      "Word 11 (\"product\") appears 3 time.\n",
      "Word 12 (\"qualiti\") appears 1 time.\n",
      "Word 13 (\"smell\") appears 1 time.\n",
      "Word 14 (\"stew\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_0 = bow_corpus[0]\n",
    "\n",
    "for i in range(len(bow_doc_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "                                                     dictionary[bow_doc_0[i][0]], \n",
    "                                                     bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.043*\"coffe\" + 0.026*\"flavor\" + 0.020*\"tast\" + 0.018*\"like\" + 0.013*\"good\" + 0.012*\"love\" + 0.011*\"drink\" + 0.011*\"product\" + 0.009*\"great\" + 0.009*\"tri\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.020*\"like\" + 0.016*\"tast\" + 0.014*\"product\" + 0.014*\"good\" + 0.012*\"great\" + 0.010*\"love\" + 0.009*\"flavor\" + 0.008*\"food\" + 0.007*\"time\" + 0.007*\"order\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=2, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\\n\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.012*\"coffe\" + 0.009*\"flavor\" + 0.009*\"tast\" + 0.007*\"good\" + 0.006*\"product\" + 0.005*\"like\" + 0.005*\"vanilla\" + 0.005*\"love\" + 0.005*\"tri\" + 0.005*\"drink\"\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.017*\"coffe\" + 0.008*\"flavor\" + 0.007*\"like\" + 0.007*\"great\" + 0.007*\"love\" + 0.006*\"cup\" + 0.006*\"tast\" + 0.006*\"order\" + 0.005*\"good\" + 0.005*\"product\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8525947332382202\t \n",
      "Topic 1: 0.020*\"like\" + 0.016*\"tast\" + 0.014*\"product\" + 0.014*\"good\" + 0.012*\"great\" + 0.010*\"love\" + 0.009*\"flavor\" + 0.008*\"food\" + 0.007*\"time\" + 0.007*\"order\"\n",
      "\n",
      "Score: 0.14740528166294098\t \n",
      "Topic 0: 0.043*\"coffe\" + 0.026*\"flavor\" + 0.020*\"tast\" + 0.018*\"like\" + 0.013*\"good\" + 0.012*\"love\" + 0.011*\"drink\" + 0.011*\"product\" + 0.009*\"great\" + 0.009*\"tri\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[45]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic {}: {}\".format(score, index, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9146600961685181\t \n",
      "Topic 1: 0.017*\"coffe\" + 0.008*\"flavor\" + 0.007*\"like\" + 0.007*\"great\" + 0.007*\"love\" + 0.006*\"cup\" + 0.006*\"tast\" + 0.006*\"order\" + 0.005*\"good\" + 0.005*\"product\"\n",
      "\n",
      "Score: 0.08533991128206253\t \n",
      "Topic 0: 0.012*\"coffe\" + 0.009*\"flavor\" + 0.009*\"tast\" + 0.007*\"good\" + 0.006*\"product\" + 0.005*\"like\" + 0.005*\"vanilla\" + 0.005*\"love\" + 0.005*\"tri\" + 0.005*\"drink\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic {}: {}\".format(score, index, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for text in bow_corpus:\n",
    "    index, score = sorted(lda_model[text], key=lambda tup: -1*tup[1])[0]\n",
    "    classes.append(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = [\" \".join(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_text, classes, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Use neural networks to predict the sentiment of bodies of text. These are across two topics which were generated \n",
    "\n",
    "## Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 100, 100)          1023200   \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 1,033,201\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 1,023,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 0.6443 - acc: 0.5922 - val_loss: 0.6338 - val_acc: 0.5975\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 0.6237 - acc: 0.6142 - val_loss: 0.6235 - val_acc: 0.6313\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 0.6132 - acc: 0.6538 - val_loss: 0.6373 - val_acc: 0.6744\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 0.6143 - acc: 0.6819 - val_loss: 0.6111 - val_acc: 0.6844\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 0.6064 - acc: 0.6927 - val_loss: 0.6128 - val_acc: 0.6756\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 6s 124ms/step - loss: 0.6090 - acc: 0.6903 - val_loss: 0.6152 - val_acc: 0.6769\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.6128 - acc: 0.6835\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 100, 100)          1023200   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,087,457\n",
      "Trainable params: 64,257\n",
      "Non-trainable params: 1,023,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.6069 - acc: 0.6680 - val_loss: 0.5413 - val_acc: 0.7206\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.4783 - acc: 0.7798 - val_loss: 0.5118 - val_acc: 0.7456\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.4148 - acc: 0.8292 - val_loss: 0.4933 - val_acc: 0.7575\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.3599 - acc: 0.8709 - val_loss: 0.4744 - val_acc: 0.7656\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.3108 - acc: 0.8963 - val_loss: 0.4608 - val_acc: 0.7719\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.2647 - acc: 0.9273 - val_loss: 0.4702 - val_acc: 0.7756\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4747 - acc: 0.7675\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrant NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.2273 - acc: 0.9473 - val_loss: 0.4484 - val_acc: 0.7919\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.1876 - acc: 0.9667 - val_loss: 0.4560 - val_acc: 0.7862\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.1543 - acc: 0.9795 - val_loss: 0.4453 - val_acc: 0.7925\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.1270 - acc: 0.9892 - val_loss: 0.4476 - val_acc: 0.7900\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.1056 - acc: 0.9931 - val_loss: 0.4513 - val_acc: 0.7906\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.0881 - acc: 0.9970 - val_loss: 0.4561 - val_acc: 0.7944\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4400 - acc: 0.7930\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
